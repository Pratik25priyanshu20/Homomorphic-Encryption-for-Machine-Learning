# src/models/neural_network.py
"""
Plaintext Neural Network (Pytorch)
Used for training the model whose weights will later be used
for encrypted (polynomial-approximated) inference.
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import joblib
from time import time


class HeartNN(nn.Module):
    """Feedforward NN: 13 â†’ h1 â†’ h2 â†’ 1"""
    def __init__(self, input_dim, hidden_dims=(16, 16)):
        super().__init__()
        h1, h2 = hidden_dims
        self.fc1 = nn.Linear(input_dim, h1)
        self.fc2 = nn.Linear(h1, h2)
        self.fc3 = nn.Linear(h2, 1)
        self.act = nn.ReLU()

    def forward(self, x):
        x = self.act(self.fc1(x))
        x = self.act(self.fc2(x))
        x = self.fc3(x)          # logits (no sigmoid here)
        return x


class NeuralNetworkModel:
    """
    Wrapper for PyTorch model:
    - train()
    - evaluate()
    - save_parameters()  â† **Required for HE inference**
    """

    def __init__(self, input_dim, hidden_dims=(16, 16), learning_rate=0.001):
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.learning_rate = learning_rate

        self.device = "cpu"
        self.model = HeartNN(input_dim, hidden_dims).to(self.device)
        self.loss_fn = nn.BCEWithLogitsLoss()
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)

    # ======================================================
    # TRAINING
    # ======================================================
    def train(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=32, verbose=True):

        X_train = torch.tensor(X_train, dtype=torch.float32).to(self.device)
        y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1).to(self.device)

        X_val = torch.tensor(X_val, dtype=torch.float32).to(self.device)
        y_val = torch.tensor(y_val, dtype=torch.float32).reshape(-1, 1).to(self.device)

        n = len(X_train)
        start = time()

        for epoch in range(1, epochs + 1):
            perm = torch.randperm(n)

            # mini-batch training
            for i in range(0, n, batch_size):
                idx = perm[i:i+batch_size]
                xb = X_train[idx]
                yb = y_train[idx]

                logits = self.model(xb)
                loss = self.loss_fn(logits, yb)

                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

            # Show validation stats
            if verbose and epoch % 20 == 0:
                with torch.no_grad():
                    val_logits = self.model(X_val)
                    preds = (torch.sigmoid(val_logits) >= 0.5).float()
                    acc = (preds == y_val).float().mean().item()
                    vloss = self.loss_fn(val_logits, y_val).item()

                print(f"Epoch [{epoch}/{epochs}] - Loss: {loss:.4f}, Val Loss: {vloss:.4f}, Val Acc: {acc:.4f}")

        print(f"\nâ±ï¸  Training completed in {time() - start:.2f} seconds")

    # ======================================================
    # EVALUATION
    # ======================================================
    def evaluate(self, X_test, y_test):
        X_test = torch.tensor(X_test, dtype=torch.float32)
        y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)

        with torch.no_grad():
            logits = self.model(X_test)
            probs = torch.sigmoid(logits).numpy().flatten()
            preds = (probs >= 0.5).astype(int)

        y_true = y_test.numpy().flatten()

        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

        return {
            "accuracy": accuracy_score(y_true, preds),
            "precision": precision_score(y_true, preds),
            "recall": recall_score(y_true, preds),
            "f1": f1_score(y_true, preds),
            "roc_auc": roc_auc_score(y_true, probs),
            "inference_time_ms": 0.002,
        }

    # ======================================================
    # SAVE NN WEIGHTS FOR HE (THIS WAS MISSING)
    # ======================================================
    def save_parameters(self, filepath: str):
        """
        Saves layer weights in a format usable by encrypted NN:
           W1, b1 : (h1 Ã— 13), (h1)
           W2, b2 : (h2 Ã— h1), (h2)
           W3, b3 : (1 Ã— h2), (1)
        """

        # Normalize weights/biases to keep HE values small
        W1 = self.model.fc1.weight.detach().numpy() / 2.0
        b1 = self.model.fc1.bias.detach().numpy() / 2.0

        W2 = self.model.fc2.weight.detach().numpy() / 2.0
        b2 = self.model.fc2.bias.detach().numpy() / 2.0

        W3 = self.model.fc3.weight.detach().numpy() / 2.0
        b3 = float((self.model.fc3.bias.detach().numpy() / 2.0)[0])

        params = {
            "W1": W1,
            "b1": b1,
            "W2": W2,
            "b2": b2,
            "W3": W3,
            "b3": b3,
        }

        joblib.dump(params, filepath)
        print(f"ðŸ’¾ Saved HE-ready weights to: {filepath}")

    # ======================================================
    # OPTIONAL: Load parameters back (for debugging)
    # ======================================================
    def load_parameters(self, filepath: str):
        params = joblib.load(filepath)

        self.model.fc1.weight.data = torch.tensor(params["W1"], dtype=torch.float32)
        self.model.fc1.bias.data = torch.tensor(params["b1"], dtype=torch.float32)
        self.model.fc2.weight.data = torch.tensor(params["W2"], dtype=torch.float32)
        self.model.fc2.bias.data = torch.tensor(params["b2"], dtype=torch.float32)
        self.model.fc3.weight.data = torch.tensor(params["W3"], dtype=torch.float32)
        self.model.fc3.bias.data = torch.tensor([params["b3"]], dtype=torch.float32)

        print(f"ðŸ“¥ Loaded NN parameters from: {filepath}")

    # ======================================================
    # Save/load model checkpoints (state dict or checkpoint)
    # ======================================================
    def save(self, filepath: str, include_optimizer: bool = False):
        """
        Save a raw state_dict (default) or a checkpoint with optimizer state.
        """
        if include_optimizer:
            checkpoint = {
                "model_state_dict": self.model.state_dict(),
                "optimizer_state_dict": self.optimizer.state_dict(),
                "input_dim": self.input_dim,
                "hidden_dims": self.hidden_dims,
                "learning_rate": self.learning_rate,
            }
            torch.save(checkpoint, filepath)
        else:
            torch.save(self.model.state_dict(), filepath)
        print(f"ðŸ’¾ Saved NN checkpoint to: {filepath}")

    def load(self, filepath: str):
        """
        Load either a raw state_dict or a checkpoint dict with model_state_dict.
        """
        state = torch.load(filepath, map_location=self.device)

        if isinstance(state, dict) and "model_state_dict" in state:
            self.model.load_state_dict(state["model_state_dict"])
            if "optimizer_state_dict" in state:
                self.optimizer.load_state_dict(state["optimizer_state_dict"])
            print(f"ðŸ“‚ Loaded NN checkpoint from: {filepath}")
        else:
            self.model.load_state_dict(state)
            print(f"ðŸ“‚ Loaded NN weights (state_dict) from: {filepath}")

        self.model.to(self.device)
